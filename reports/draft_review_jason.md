# Question:  What is your understanding of the experiment the team is replicating?  What question does it answer?  How clear is the team's explanation?

My understanding is that Alana and Sam’s experiment is that they want to be able to replicate Paterson’s Worms in python, get as close as possible to the original ruleset using the constraints of networkx. The question it answers I’m assuming is whether or not the worm patterns generated length will have a long-tailed distribution. 

# Methodology: Do you understand the methodology?  Does it make sense for the question?  Are there limitations you see that the team did not address?
	
I understand the methodology pretty well. This is mostly because this is a pre-existing model and they seem to have already built the foundation for it. They mentioned that NetworkX's built in triangular grid has all the features they need, so I'm confident there shouldn't be any big limitations in their way. 

# Results: Do you understand what the results are (not yet considering their interpretation)?  If they are presented graphically, are the visualizations effective?  Do all figures have labels on the axes and captions?
	
At the moment, I somewhat understand what the results are. It looks like they’ve already tried to generate patterns, and how neighboring nodes within the visualization are dependent on previous states. They framed it pretty effectively, and said they haven’t done quantitative analysis now. 

# Interpretation: Does the draft report interpret the results as an answer to the motivating question?  Does the argument hold water?
	
The draft report doesn’t quite interpret the results as an answer to the opening answer as they haven’t gotten far enough to conclude anything. What they mentioned about nodes and neighboring nodes does seem sound though, and I’m curious to learn more.

# Replication: Are the results in the report consistent with the results from the original paper?  If so, how did the authors demonstrate that consistency?  Is it quantitative or qualitative?
	
From what I can tell, the report isn’t complete enough for me to guarantee parity. Though I will say, if they are able to replicate Paterson’s Worms, there should be consistency inherently. If done exactly the same, the consistency should be consistent both quantitatively and qualitatively. 

# Extension: Does the report explain an extension to the original experiment clearly?  Can it answer an interesting question that the original experiment did not answer?
	
I think they do explain the extension pretty clearly, given the context they provide earlier in the report. They want to check if the lengths of the worms will form a heavy-tailed distribution if they analyze it. I would mention that it would be useful for them to explain what a heavy-tailed distribution would represent in the context of the experiment, for broader audiences. 

# Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?
	
I think the team is in a really good spot with substantial progress! The background info is made clear, the implementation is sound, and the experiment makes sense and has purpose. The extension is a good next step and the team seems to be confident in being able to accomplish their project, if time permits. 
  
